{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5834303c-298c-444c-b33c-5a0a335f8719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pierr\\environment\\RL\\lib\\site-packages\\gymnasium\\core.py:27: UserWarning: \u001b[33mWARN: Gymnasium minimally supports python 3.6 as the python foundation not longer supports the version, please update your version to 3.7+\u001b[0m\n",
      "  \"Gymnasium minimally supports python 3.6 as the python foundation not longer supports the version, please update your version to 3.7+\"\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "from inspect import getsource\n",
    "import torch\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "from drl_nav.utils.context import Context\n",
    "from drl_nav.config.config import AgentConfiguration\n",
    "from drl_nav.agent import Agent_PER, Agent_DQN_pixel\n",
    "from drl_nav.network.heads import LabelizerNet, AuxNet, QNet\n",
    "from drl_nav.network.bodies import ConvBody"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96651797-f52c-4323-8ac9-8530d88c0b93",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f85c482-a37f-4f32-abc3-8ffedc0f7dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size: 4\n",
      "action space: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "\n",
    "if len(env.observation_space.shape):\n",
    "    state_size = env.observation_space.shape[0]\n",
    "else:\n",
    "    state_size = env.observation_space.shape\n",
    "    \n",
    "if isinstance(env.action_space, gym.spaces.discrete.Discrete):\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "print(f\"state size: {state_size}\")    \n",
    "print(f\"action space: {env.action_space}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8bedf-25ee-4628-af58-9b38b566893e",
   "metadata": {},
   "source": [
    "#### Random interaction loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb2ed6e1-5f6d-489c-908f-a46a7bef9785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01263359 -0.00914562 -0.01324817 -0.0302738 ]\n"
     ]
    }
   ],
   "source": [
    "observation, info = env.reset()\n",
    "print(observation)\n",
    "action = env.action_space.sample()\n",
    "observation, reward, terminated, truncated, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73b54130-1e15-4244-b9e0-3c3821794246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.0\n",
      "20.0\n",
      "17.0\n",
      "14.0\n",
      "23.0\n"
     ]
    }
   ],
   "source": [
    "N_EPISODES = 5\n",
    "render = None # \"human\"\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode=None)\n",
    "\n",
    "#for i in tqdm(range(1, N_EPISODES + 1)):\n",
    "for i in range(1, N_EPISODES + 1):\n",
    "    \n",
    "    score, step = 0, 0\n",
    "    state, _ = env.reset()\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated | truncated\n",
    "        \n",
    "        score += reward\n",
    "        step += 1\n",
    "        \n",
    "        if (done | (step > 200)):\n",
    "            print(score)\n",
    "            break\n",
    "            \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8030d9e-afe8-4a20-a8d0-74feb983eb4a",
   "metadata": {},
   "source": [
    "#### Interaction loop with agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e59126ad-3ee5-423c-9f0e-ab720144f477",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AgentConfiguration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc081218-7fd7-4c9a-aaf1-0e29ae5c2721",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AgentConfiguration' object has no attribute 'network'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-c0464218a1fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'AgentConfiguration' object has no attribute 'network'"
     ]
    }
   ],
   "source": [
    "config.network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a8d050c-dd08-417b-9ae9-176091259e69",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "create() missing 2 required positional arguments: 'args' and 'kwargs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-1af4523370be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAgentConfiguration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAgent_DQN_pixel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\DataScience\\Projets\\DRL\\DRL_navigation\\drl_nav\\agent\\agent_DQN_pixel.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, context, config)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExponentialSchedule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minstantiate_from_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mnetwork_body\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNetworkCreator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork_body\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq_network_local\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mQNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork_body\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork_head\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         self.q_network_target = QNet(\n",
      "\u001b[1;31mTypeError\u001b[0m: create() missing 2 required positional arguments: 'args' and 'kwargs'"
     ]
    }
   ],
   "source": [
    "context = Context(state_size=state_size, action_size=action_size)\n",
    "config = AgentConfiguration()\n",
    "\n",
    "agent = Agent_DQN_pixel(context, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319dbce3-fa8e-4db1-b28b-d9298a5df867",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f068c9-2ad6-484a-b9c1-f846c35ddf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPISODES = 5\n",
    "render = None # \"human\"\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode=None)\n",
    "\n",
    "#for i in tqdm(range(1, N_EPISODES + 1)):\n",
    "for i in range(1, N_EPISODES + 1):\n",
    "    \n",
    "    score, step = 0, 0\n",
    "    state, _ = env.reset()\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated | truncated\n",
    "        \n",
    "        score += reward\n",
    "        step += 1\n",
    "        \n",
    "        if (done | (step > 200)):\n",
    "            print(score)\n",
    "            break\n",
    "            \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cc08f1-20e9-4995-bd21-f5113fae58ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2728594a-b810-4f08-98cb-2e35d7f40192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32fb48f-51a4-447a-af20-8562b63cfb5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc394353-ea50-4452-bfa8-0fa67c7ef928",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPISODES = 5\n",
    "\n",
    "for i in tqdm(range(1, N_EPISODES + 1)):\n",
    "    \n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    state = env_info.visual_observations[0]    \n",
    "    score, step = 0, 0\n",
    "    errors_color_detection_episode = []\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        action = agent.act(state)\n",
    "        \n",
    "        env_info = env.step(action)[brain_name]      \n",
    "        next_state = env_info.visual_observations[0] \n",
    "        reward = env_info.rewards[0]                 \n",
    "        done = env_info.local_done[0]    \n",
    "        \n",
    "        agent.step(state, action, reward, next_state, done)\n",
    "        \n",
    "        score += reward                               \n",
    "        state = next_state  \n",
    "        step += 1\n",
    "        \n",
    "        if (done | (step > 200)):\n",
    "            scores.append(score)\n",
    "            errors_color_detection.append(np.mean(errors_color_detection_episode))\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
